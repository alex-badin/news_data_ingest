{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Updating pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexbadin/miniconda3/envs/db_prep/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import openai\n",
    "import pinecone\n",
    "from telethon import TelegramClient\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "keys_path = '../keys/'\n",
    "data_path = '../../TG_messages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(keys_path+'api_keys.json') as f:\n",
    "  data = json.loads(f.read())\n",
    "\n",
    "# load TG credentials\n",
    "api_id = data['api_id'] \n",
    "api_hash = data['api_hash']\n",
    "phone = data['phone']\n",
    "\n",
    "#load openai credentials\n",
    "openai_key = data['openai_key']\n",
    "\n",
    "# load pinecone credentials\n",
    "pine_key = data['pine_key']\n",
    "pine_env = data['pine_env']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "1) Identify which data to download:\n",
    "- by date\n",
    "- by id\n",
    "Anyway need to store last date or id. So let's keep it last_id.\n",
    "\n",
    "2) Remove duplicates in pinecone \n",
    "- they should not be there as id is exactly channel + message_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps (per each channel):\n",
    "- identify last_id (channels.csv)\n",
    "- download from TG as per last_id\n",
    "- process messages: cleaning, deduplicating, summary\n",
    "- create embeds from openai\n",
    "- date format into int\n",
    "- transform into pinecone format\n",
    "- upsert into pinecone\n",
    "- add into main files (pkl) - optional\n",
    "- iterate over channels\n",
    "- update last_id in channels.csv\n",
    "- create session_stats file\n",
    "- update total_stats file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Unicode range for emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji_pattern.sub(r'', str(text))\n",
    "\n",
    "    # Regular expression for URLs\n",
    "    url_pattern = re.compile(r\"http\\S+|www\\S+\")\n",
    "        \n",
    "    # Remove URLs\n",
    "    text = url_pattern.sub(r'', str(text))\n",
    "    \n",
    "    # Remove any remaining variation selectors\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "    #Remove Foreign Agent text    \n",
    "    pattern = re.compile(r'[А-ЯЁ18+]{3,}\\s[А-ЯЁ()]{5,}[^\\n]*ИНОСТРАННОГО АГЕНТА')\n",
    "    text = pattern.sub('', text)\n",
    "    name1 = 'ПИВОВАРОВА АЛЕКСЕЯ ВЛАДИМИРОВИЧА'\n",
    "    text = text.replace(name1, '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the news (select 2 most important sentences)\n",
    "def summarize(text, language=\"russian\", sentences_count=2):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    return ' '.join([str(sentence) for sentence in summary])\n",
    "\n",
    "# NEED MORE FLEXIBLE MODEL\n",
    "# summarize the news - need to keep length upto 750 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_messages(df, channel, stance):\n",
    "    df = df.drop_duplicates(subset=['id']).copy() # create a copy of the DataFrame before modifying it\n",
    "    df.loc[:, 'cleaned_message'] = df['message'].apply(clean_text) #remove emojis, urls, foreign agent text\n",
    "    df = df[~df.cleaned_message.str.len().between(0, 30)] #remove empty or too short messages\n",
    "    # summarize cleaned_messages: 2 sentences if length > 750, 3 sentences if length > 1500\n",
    "    df.loc[:, 'summary'] = df['cleaned_message'].apply(lambda x: summarize(x, sentences_count=3) if len(x) > 750 else summarize(x, sentences_count=2) if len(x) > 500 else x)\n",
    "    # add channel name & stance\n",
    "    df.loc[:, 'channel'] = channel\n",
    "    df.loc[:, 'stance'] = stance\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get new messages from channel\n",
    "async def get_new_messages(channel, last_id, stance):\n",
    "    async with TelegramClient('session', api_id, api_hash) as client:\n",
    "        # COLLECT NEW MESSAGES\n",
    "        data = [] # for collecting new messages\n",
    "        # check if last_id is integer (=set)\n",
    "        try:\n",
    "            offset_id = int(last_id)\n",
    "        except:\n",
    "            offset_id = 0\n",
    "        async for message in client.iter_messages(channel, reverse=True, offset_id=offset_id):\n",
    "            data.append(message.to_dict())\n",
    "        # if no new messages, skip\n",
    "    print(f\"Channel: {channel}, N of new messages: {len(data)}\")\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    # create df from collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    # process new messages\n",
    "    df = process_new_messages(df, channel, stance)\n",
    "    # return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for openai embeddings\n",
    "def get_embeddings(text, model=\"text-embedding-ada-002\"):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_pinecone(df, index, batch_size=100):\n",
    "    # convert date to integer (without time)\n",
    "    df['date'] = df['date'].apply(lambda x: int(time.mktime(x.timetuple())))\n",
    "    # id as channel_id + message_id\n",
    "    df['id'] = df['channel'] + '_' + df['id'].astype(str)\n",
    "    # convert to pinecone format\n",
    "    df['metadata'] = df[['cleaned_message', 'summary', 'stance', 'channel', 'date', 'views']].to_dict('records')\n",
    "    df = df[['id', 'values', 'metadata']]\n",
    "    bath_size = batch_size\n",
    "    for i in range(0, df.shape[0], bath_size):\n",
    "        index.upsert(vectors=df.iloc[i:i+bath_size].to_dict('records'))\n",
    "    print(f\"Upserted {df.shape[0]} records. Last id: {df.iloc[-1]['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init openai\n",
    "openai.api_key = openai_key\n",
    "# initialize pinecone\n",
    "pinecone.init(api_key=pine_key, environment=pine_env)\n",
    "index_name='tg-news'\n",
    "index = pinecone.Index(index_name)\n",
    "# create session_stats\n",
    "df_channel_stats = pd.DataFrame() # store N of posts per channel per day\n",
    "session_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # to name session stats file\n",
    "\n",
    "# ITERATE OVER CHANNELS (df_channels) TO UPDATE PINCONE INDEX\n",
    "df_channels = pd.read_csv('/Users/alexbadin/Library/CloudStorage/GoogleDrive-alex.badin@gmail.com/My Drive/Colab Notebooks/Narratives/notebooks_data_ingest/channels.csv', sep = ';')\n",
    "for i, channel, last_id, stance in tqdm_notebook(df_channels[['channel_name', 'last_id', 'stance']].itertuples(), total=df_channels.shape[0]):\n",
    "    # get & clean new messages\n",
    "    df = await get_new_messages(channel, last_id, stance)\n",
    "    if df is None:\n",
    "        continue\n",
    "    # get embeddings\n",
    "    df.loc[:, 'values'] = df['cleaned_message'].apply(get_embeddings)\n",
    "    print(f\"Embeddings for {df.shape[0]} messages collected.\")\n",
    "    # upsert to pinecone\n",
    "    upsert_to_pinecone(df, index)\n",
    "\n",
    "    # save session stats for channel\n",
    "    df_channel_stats[channel] = df['date'].dt.date.value_counts()\n",
    "    df_channel_stats.to_csv(f'../session_stats/channel_stats_{session_time}.csv', sep=';', index=True)\n",
    "\n",
    "    # update last_id in df_channels\n",
    "    df_channels.loc[i, 'last_id'] = df['id'].max()\n",
    "    df_channels.to_csv('channels.csv', index=False, sep=';')\n",
    "    # save df to pickle\n",
    "    df_old = pd.read_pickle(data_path + channel + '.pkl')\n",
    "    df_new = pd.concat([df_old, df])\n",
    "    df_new.to_pickle(data_path + channel + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.41813,\n",
       " 'namespaces': {'': {'vector_count': 41813}},\n",
       " 'total_vector_count': 41813}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
